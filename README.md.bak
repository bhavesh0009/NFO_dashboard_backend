# Angel One Data Pipeline

_Last Updated: March 9, 2025_

## Overview

This project extracts data from Angel One API and stores it in DuckDB for analysis and reporting. It provides a reliable data pipeline for financial market data.

## Features

- ✅ Secure connection to Angel One API
- ✅ Data extraction for various financial instruments
- ✅ Token values storage (master stock records)
  - Futures & Options (F&O) token processing
    - Current expiry futures contracts
    - Current expiry options with strike prices
  - Equity spot token mapping
  - Automatic expiry date handling
  - Strike price validation and distribution analysis
  - Smart token refresh (avoids unnecessary API calls)
- ✅ Efficient storage in DuckDB database
  - Unified schema for all token types (futures, options, equity)
  - Smart token type differentiation
  - Automated data validation
  - Referential integrity between instruments
- Automated data refresh and synchronization
- ✅ Historical data processing
  - Fetches historical OHLCV data for equity tokens
  - Supports multiple intervals (ONE_MINUTE, FIVE_MINUTE, ONE_HOUR, ONE_DAY)
  - Historical data from 2010 to present
  - Batched processing with rate limiting
  - Efficient storage with duplicate handling

## Prerequisites

- Python 3.8+
- Angel One trading account with API access
- Required Python packages (see `requirements.txt`)

## Installation

1. Clone this repository
2. Create and activate virtual environment:

   ```bash
   # Create virtual environment
   python -m venv venv

   # Activate virtual environment
   # On Windows:
   source venv/Scripts/activate  # Git Bash
   # OR
   .\venv\Scripts\activate      # Command Prompt
   # OR
   .\venv\Scripts\Activate.ps1  # PowerShell

   # On Linux/Mac:
   source venv/bin/activate
   ```

3. Install required packages:

   ```bash
   pip install -r requirements.txt --upgrade
   ```

4. Configure your Angel One API credentials (see Configuration section)

## Configuration

The project uses a YAML-based configuration system for managing all settings:

### Environment Variables

Create a `.env` file in the project root with your API credentials:

```
ANGEL_ONE_CLIENT_ID=your_client_id
ANGEL_ONE_PASSWORD=your_password
ANGEL_ONE_API_KEY=your_api_key
ANGEL_ONE_PIN=your_pin
```

### Application Configuration

All other settings are managed in `config/config.yaml`:

```yaml
# API Configuration
api:
  angel_one:
    token_master_url: "https://margincalculator.angelbroking.com/OpenAPI_File/files/OpenAPIScripMaster.json"

# Market Configuration
market:
  trading_hours:
    start: "09:15"  # IST
    end: "15:30"    # IST
  pre_market:
    start: "09:00"
    end: "09:15"
  post_market:
    start: "15:30"
    end: "15:45"

# Database Configuration
database:
  default_path: "nfo_derivatives_hub.duckdb"  # Central hub for NFO derivatives data

# Additional configurations for token types, etc.
```

The database serves as a central hub for:

- Token master data (futures and equity)
- Historical price data
- Spot values for dashboard
- Market analytics and metrics

Access configuration values in code:

```python
from src.config_manager import config

# Get API URL
api_url = config.get('api', 'angel_one', 'token_master_url')

# Get market hours
market_start = config.get('market', 'trading_hours', 'start')
```

## Usage

```python
# Basic usage example
from src.token_manager import TokenManager
from src.db_manager import DBManager

# Initialize managers
db_manager = DBManager()
token_manager = TokenManager(db_manager)

# Fetch and process tokens
# The system will automatically check if tokens are already up-to-date
# and skip the refresh if they were updated after today's pre-market start time
success = token_manager.process_and_store_tokens()
print(f"Token processing {'successful' if success else 'failed'}")

# To force a hard refresh regardless of last update time
success = token_manager.process_and_store_tokens(hard_refresh=True)
print(f"Token hard refresh {'successful' if success else 'failed'}")
```

### Historical Data Processing

The system includes functionality to fetch and store historical price data for equity tokens:

```python
from src.historical_data_manager import HistoricalDataManager

# Initialize historical data manager
manager = HistoricalDataManager()

# Fetch and store historical data for 5 equity tokens (for testing)
results = manager.fetch_and_store_historical_data(limit=5)
print(f"Historical data processing: {results['success']} successful, {results['errors']} errors")
```

You can also use the main.py script with various commands:

```bash
# Test historical data fetch (without storing)
python main.py history --limit 5 --verbose

# Test with storing data for 10 tokens
python main.py history --limit 10 --store --verbose

# Process with ONE_MINUTE interval instead of default ONE_DAY
python main.py history --limit 5 --interval ONE_MINUTE --verbose

# Process all equity tokens in batches
python main.py batch

# Process with custom batch size and limit
python main.py batch --batch-size 10 --limit 50

# Process with custom interval (ONE_MINUTE, FIVE_MINUTE, etc.)
python main.py batch --batch-size 10 --limit 50 --interval ONE_HOUR

# Test connection to Angel One API
python main.py connection

# Process and store tokens
python main.py tokens

# Run all basic tests
python main.py
```

## Data Processing

The pipeline handles three main types of financial instruments in a unified storage system:

1. **Futures Tokens**
   - Filters FUTSTK instruments from NFO segment
   - Automatically identifies current expiry contracts
   - Processes expiry dates into standardized format
   - Handles numeric data validation

2. **Options Tokens**
   - Filters OPTSTK instruments from NFO segment
   - Identifies current expiry contracts
   - Validates and processes strike prices
   - Provides strike price distribution analysis
   - Maintains standardized date formats

3. **Equity Tokens**
   - Maps futures to corresponding equity spot tokens
   - Maintains referential integrity with futures
   - Stores in normalized database structure
   - Automatic type conversion and validation

## Database Utilities

The project includes utility scripts for managing the database:

### Database Utility Tool

A comprehensive utility for database management operations:

```bash
# Check database status
python utils/db_utility.py status

# Truncate all tables (will prompt for confirmation)
python utils/db_utility.py truncate

# Truncate without confirmation
python utils/db_utility.py truncate --no-confirm

# Create a backup
python utils/db_utility.py backup

# Create a labeled backup
python utils/db_utility.py backup --label pre_release

# Restore from latest backup (will prompt for confirmation)
python utils/db_utility.py restore

# Restore from specific backup file
python utils/db_utility.py restore --file db_backups/db_backup_20250309_160000.duckdb
```

You can also access these functions programmatically:

```python
from src.db_manager import DBManager

# Initialize database manager
db_manager = DBManager()

# Truncate all tables
db_manager.truncate_tables()

# Close connection when done
db_manager.close()
```

## Database Schema

The system uses a unified token master table with the following structure:

```sql
CREATE TABLE token_master (
    token VARCHAR,
    symbol VARCHAR,
    name VARCHAR,
    expiry DATE,
    strike DECIMAL(18,6),  -- Particularly important for options
    lotsize INTEGER,
    instrumenttype VARCHAR,
    exch_seg VARCHAR,
    tick_size DECIMAL(18,6),
    token_type VARCHAR,  -- 'FUTURES', 'OPTIONS', or 'EQUITY'
    futures_token VARCHAR,  -- Reference to futures token for equity
    created_at TIMESTAMP,
    PRIMARY KEY (token)
)
```

## Project Structure

```
├── config/
│   └── config.yaml          - Application configuration
├── src/
│   ├── angel_one_connector.py - Connection to Angel One API
│   ├── config_manager.py    - Configuration management
│   ├── db_manager.py        - DuckDB operations handler
│   ├── token_manager.py     - Token processing logic
│   └── __pycache__/         - Python cache files
├── utils/                   - Utility scripts for maintenance and development
│   ├── db_utility.py        - Complete database management tool (backup/restore/status/truncate)
│   ├── reset_for_testing.py - Quick database reset with auto-backup for testing
│   └── truncate_db.py       - Simple database truncation utility
├── db_backups/              - Generated directory for database backups
├── main.py                  - Application entry point
├── .env                     - Environment variables (not tracked)
├── nfo_derivatives_hub.duckdb - Default database file
├── README.md                - Project documentation
├── DEVELOPMENT_LOG.md       - Development progress tracking
└── requirements.txt         - Project dependencies
```

The project follows a modular structure:

- **src/**: Core application code
  - `angel_one_connector.py`: Handles API authentication and connection
  - `config_manager.py`: Manages application configuration with singleton pattern
  - `db_manager.py`: Handles all database operations with error recovery
  - `token_manager.py`: Processes token data with smart refresh mechanisms

- **utils/**: Utility scripts for development and maintenance
  - `db_utility.py`: Comprehensive database management tool with multiple commands:
    - `status`: View database statistics and condition
    - `backup`: Create timestamped database backups
    - `restore`: Recover database from backups
    - `truncate`: Safely clear tables with confirmation
  - `reset_for_testing.py`: One-step testing utility that creates a backup before truncating
  - `truncate_db.py`: Focused utility for safely truncating database tables

- **config/**: Configuration files
  - `config.yaml`: Centralized application settings

- **db_backups/**: Automatically created and stores timestamped database backups

## License

[MIT License](LICENSE)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
